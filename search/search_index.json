{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"About","text":"<p>Status</p> <p>Package is under development, don't install quite yet. This message will be updated when it is ready to use.</p> <p>Description</p> <p>This python package allows you to run your own self hosted and searchable archive of reddit data. Originally it also allowed you to download the reddit archives from files.pushshift.io but alas...</p> <p></p> <p>Archives</p> <p>If you saved pushshift reddit archives before they were nuked, you may still find this package useful. You can get started on the setup page. If you didn't save the archives, but you still want to use it, you can take a look at the data access page.</p>"},{"location":"citation/","title":"Citation","text":"<p>Citation</p> <p>If you use this package in an academic context, citation is appreciated but optional. Also note this project is dependent on the pushshift reddit dataset for which a separate citation is recommended.</p> BibTeX<pre><code>@online{psarch,\ntitle={psarch},\nauthor={Hart Traveller},\nyear={2023},\npublisher={GitHub},\nurl={https://github.com/harttraveller/psarch},\n}\n</code></pre>"},{"location":"cli/","title":"CLI","text":"<ul> <li>psarch</li> <li>location<ul> <li>view</li> <li>open</li> <li>change</li> </ul> </li> </ul>"},{"location":"cli/#location","title":"location","text":"<p>The location is a path to the working directory where the psarch data and elasticsearch instance is stored.</p> <p>There are three possible subcommands.</p>"},{"location":"cli/#view","title":"view","text":"<p>This allows you to view the current path to the working directory.</p>"},{"location":"cli/#open","title":"open","text":"<p>This opens the file explorer at the working directory location.</p>"},{"location":"cli/#change","title":"change","text":"<p>This allows you to update the working directory location.</p>"},{"location":"data/","title":"Data","text":"<p>If you do not have the data and would like get a copy, I might be able to provide you with a copy - or perhaps even access to a streamlined web service that replicates and greatly enhances the original functionality of pushshift, and is (maybe) still actively archiving data. However, you would have to meet all of the following criteria:</p> <ol> <li>You represent an academic institution or federal agency based in: US, CA, UK, AU, or NZ.</li> <li>You are a citizen of: US, CA, UK, AU, or NZ.</li> <li>You are willing to verify your identity.</li> </ol> <p>Please check back later for a link to an application form.</p> <p>Do I have to pay for data access?</p> <p>No, I'm happy to provide it for free to research purposes to entities acting in good faith who meet the criteria above.</p> Why not upload the data on kaggle or something analogous? <p>The reasoning for not distributing the data publicly primarily pertains to the concern for potential abuse by bad actors. With that said, it's still incredibly valuable for good actors, so Reddit probably could have handled this a lot better. If you don't meet the criteria above, you can also probably look around for other data sources though. Try visiting r/pushshift and looking around a bit.</p> What if I already have the data archives? <p>If you already have the archives, you can just use this package. It's open source and free.</p> What if I represent a company? <p>US companies or contractors with the USG that are doing research that either (A) benefits humanity or (B) pertains to national security can access the data.</p>"},{"location":"details/","title":"Details","text":""},{"location":"features/","title":"Features","text":""},{"location":"features/#implemented","title":"Implemented","text":""},{"location":"features/#roadmap","title":"Roadmap","text":""},{"location":"normalization/","title":"Normalization","text":"<p>If you would like to create a custom data ingestion pipeline, please see the psing package.</p>"},{"location":"setup/","title":"Setup","text":"<p>Deprecated Note</p> <p>Originally this note read:</p> <p>Make sure you have enough storage space for the data archives. The amount of storage required will vary based on how much data you download and how far back you want to go, but I recommend a minimum of 100GB. The full uncompressed archive of comments and submissions is somewhere around ~14TB, though the exact value is uncertain.</p> <p>However, the pushshift archives are down now. Accordingly this package no longer contains the functionality to download them. You will have to find them on your own. You can visit the data access page to check you meet the requirements for the cleaned datasets/service I maintain.</p> <p>Device</p> <p>This package has only been tested on MacOS. If you run into a bug it may be attributable to this.</p> <p>Support</p> <p>If you run into a bug, please submit a GitHub Issue. I will try to address it time permitting.</p>"},{"location":"setup/#dependencies","title":"Dependencies","text":"<p>Software: Prerequisites</p> <p>Recommended installation before continuing.</p> <ul> <li>miniconda</li> </ul> Software: Automated <p>Installed via included CLI during the setup process. You can install separately if you prefer.</p> <ul> <li>elasticsearch</li> </ul> Python Packages <p>These are installed automatically. Package dependencies vary based on whether you install the package as a developer or a user. For the former, check the requirements.txt file. For the latter, check the setup.py file. The developer dependencies are a superset of the user dependencies.</p>"},{"location":"setup/#installation","title":"Installation","text":""},{"location":"setup/#user-installation","title":"User Installation","text":"<p>First, install the psarch package using pip.</p> <pre><code>pip install psarch\n</code></pre> <p>When you install the package, it creates a hidden folder in your home directory: <code>.psarch</code>. This is where the elasticsearch instance is stored. If you would like to change the location data is cached, you can use the CLI command.</p> <pre><code>psarch location update\n</code></pre> <p>Download elasticsearch. You can do this with the following command. If you have set your cache directory, elasticsearch v8.7.0 will be downloaded in the <code>&lt;cache&gt;/elasticsearch-8.7.0</code> directory.</p> Using Docker <p>You can also use docker to set up elasticsearch, as documented here. The caveat with this is that (as I understand) the docker container will operate in memory while active, and thus the amount of data you can handle will be restricted to a fraction of the RAM available on your system.</p> Supported Systems <p>At the moment I've only implemented the download wizard for elasticsearch for MacOS. If you are on a different system, you can download it manually here and move the unpacked folder into your cache directory. There may still be issues, but if you submit them on GitHub I will try to address them when I have time.</p> <pre><code>psarch elastic download\n</code></pre> <p>Once the elasticsearch download is complete, run the following command to test it and ensure it is working properly.</p> <pre><code>psarch elastic test\n</code></pre> <p>If it is not working properly, then running the following command and changing the security policy to false may fix the issue, though it also disables security features in elasticsearch. This should not be an issue if you are only running this locally.</p> <pre><code>psarch elastic security\n</code></pre> <p>After changing the security policy, attempt the test again. If it works, continue, if it does not work please submit a GitHub issue.</p> <p>The next step involves actually downloading the data. To reduce the stress on the pushshift servers, I've processed the data and uploaded normalized and filtered datasets to kaggle. These datasets are smaller, however there is information loss. If you would like to download the original data, and parse it with a custom data ingestion pipeline, interfaces are available - details on which are included on the advanced usage page.</p> <pre><code>psarch download\n</code></pre> <p>You will have to select the years for which you want to download the data.</p> <p>Finally, when you have finished downloading the data, you can start the elasticsearch instance and ingest the data into the instance.</p> <pre><code>psarch ingest\n</code></pre>"},{"location":"setup/#developer-installation","title":"Developer Installation","text":""},{"location":"cache/sort/random/","title":"Random","text":"<p>view|open|update</p> <p>Note</p> <p>The setup wizard process will take anywhere from a few minutes to a few days, depending on the following factors:</p> <ol> <li>Your internet speed.</li> <li>The processing power of your computer.</li> <li>The read/write speed of the disk your working directory is stored on.</li> </ol> <p>You can leave the process running in the background.</p>"},{"location":"cache/sort/random/#usage","title":"Usage","text":"<p>At the moment, you will need to start two independent processes in order to use the interface.</p>"},{"location":"cache/sort/random/#details","title":"Details","text":""},{"location":"cache/sort/random/#elasticsearch","title":"Elasticsearch","text":"<p>https://www.elastic.co/downloads/elasticsearch</p> <p>Note that at the moment you must disable the security features in the <code>config/elasticsearch.yml</code> file. I don't expect this should be an issue, as this is purely meant to be a locally hosted instance.</p> <pre><code># Enable security features\nxpack.security.enabled: false\n</code></pre> <p>While I have not tested it, I do not expect that using docker would work in this case, unless you have hundreds of GB of RAM. If you do</p>"},{"location":"cache/sort/random/#storage","title":"Storage","text":"<p>Also, you will need to have a storage drive with sufficient capacity to store the archive. The original pushshift archive of comments and submissions is ~2TB when compressed, and ~14TB uncompressed. The archive size has been reduced</p>"}]}